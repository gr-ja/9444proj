{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d018716b-0cb0-43f3-abfd-ace6f613534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from tokenizers import Tokenizer, models, trainers\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e5d4d27-6fcc-4e38-ad99-5aedad8842fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = load_dataset(\"dair-ai/emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0315fef-7e42-4278-8dd5-6078ec8b754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c66816cb-e29d-4940-95bc-b785747447e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = emotions[\"train\"]\n",
    "validation_data = emotions[\"validation\"]\n",
    "test_data = emotions[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25b2a4d5-ea1c-4196-844c-fddd2a6f2259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "vocab_n = 5000\n",
    "sequence_len = 64\n",
    "\n",
    "# Initialize a tokenizer using BPE (Byte Pair Encoding)\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.enable_padding(length=sequence_len)\n",
    "tokenizer.enable_truncation(max_length=sequence_len)\n",
    "tokenizer_trainer = trainers.BpeTrainer(vocab_size=vocab_n)\n",
    "tokenizer.train_from_iterator(train_data[\"text\"], trainer=tokenizer_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8b3d861-7e4f-48d3-b643-06e9b9a88405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, tokenizer: Tokenizer):\n",
    "    \"\"\" \n",
    "    Helper function to tokenize text and return corresponding token IDs as tensors.\n",
    "\n",
    "    Args:\n",
    "        text, str: Text instance from training data.\n",
    "        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.\n",
    "    Returns:\n",
    "        Tensor: One-dimensional PyTorch tensor with token IDs.\n",
    "    \"\"\"\n",
    "    return torch.tensor(tokenizer.encode(text).ids)\n",
    "\n",
    "\n",
    "def preprocess_label(label: int):\n",
    "    \"\"\" \n",
    "    Helper function to return label as tensor.\n",
    "\n",
    "    Args:\n",
    "        label, int: Label from instance.\n",
    "    Returns:\n",
    "        Tensor: One-dimensional PyTorch tensor containing the label index.\n",
    "    \"\"\"\n",
    "    return torch.tensor(label)\n",
    "\n",
    "\n",
    "def preprocess(data: dict, tokenizer: Tokenizer):\n",
    "    \"\"\" \n",
    "    Transforms input dataset to tokenized vector representations.\n",
    "\n",
    "    Args:\n",
    "        data, dict: Dictionary with text instances and labels.\n",
    "        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.\n",
    "    Returns:\n",
    "        list: List with tensors for the input texts and labels.\n",
    "    \"\"\"\n",
    "    instances = []\n",
    "\n",
    "    for text, label in zip(data[\"text\"], data[\"label\"]):\n",
    "        input = preprocess_text(text, tokenizer)\n",
    "        label = preprocess_label(label)\n",
    "        \n",
    "        instances.append((input, label))\n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85779c8a-cb93-472c-8e61-8e7aadf46fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances = preprocess(train_data, tokenizer)\n",
    "val_instances = preprocess(validation_data, tokenizer)\n",
    "test_instances = preprocess(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba01debf-56ad-4302-aa85-8b506e686a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching\n",
    "\n",
    "def batching(instances: list, batch_size: int, shuffle: bool):\n",
    "    \"\"\" \n",
    "    Batches input instances along the given size and returns list of batches.\n",
    "\n",
    "    Args:\n",
    "        instances, list: List of instances, containing a tuple of two tensors \n",
    "            for each text as well as corresponding label.\n",
    "        batch_size, int: Size for batches.\n",
    "        shuffle, bool: If true, the instances will be shuffled before batching.\n",
    "    Returns:\n",
    "        list: List containing tuples that correspond to single batches.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        random.shuffle(instances)\n",
    "\n",
    "    batches = []\n",
    "\n",
    "    # We iterate through the instances with batch_size steps\n",
    "    for i in range(0, len(instances), batch_size):\n",
    "\n",
    "        # Stacking the instances with dim=0 (default value)\n",
    "        batch_texts = torch.stack(\n",
    "            [instance[0] for instance in instances[i : i + batch_size]]\n",
    "        )\n",
    "        batch_labels = torch.stack(\n",
    "            [instance[1] for instance in instances[i : i + batch_size]]\n",
    "        )\n",
    "\n",
    "        batches.append((batch_texts, batch_labels))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5aa7e1e-9735-4c8d-8761-68d251b2036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Network\n",
    "\n",
    "class CNN_Classifier(nn.Module):\n",
    "    \"\"\" \n",
    "    CNN for sentiment classification with 6 classes, consisting of an embedding \n",
    "    layer, two convolutional layers with different filter sizes, different \n",
    "    pooling sizes, as well as one linear output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # We can implement embeddings as a simple lookup-table for given word \n",
    "        # indices\n",
    "        self.embedding = nn.Embedding(tokenizer.get_vocab_size(), 300)\n",
    "\n",
    "        # One-dimensional convolution-layer with 300 input channels, and 100  \n",
    "        # output channels as well as kernel size of 3; note that the\n",
    "        # one-dimensional convolutional layer has 3 dimensions\n",
    "        self.conv_1 = nn.Conv1d(300, 100, 3, padding=\"same\")\n",
    "\n",
    "        # Pooling with with a one-dimensional sliding window of length 3, \n",
    "        # reducing in this fashion the sequence length \n",
    "        self.pool_1 = nn.MaxPool1d(3)\n",
    "\n",
    "        # The input will be the reduced number of maximum picks from the\n",
    "        # previous operation; the dimension of those picks is the same as the\n",
    "        # output channel size from self.conv_1. We apply a different filter of \n",
    "        # size 5.\n",
    "        self.conv_2 = nn.Conv1d(100, 50, 5, padding=\"same\")\n",
    "\n",
    "        # Pooling with window size of 5\n",
    "        self.pool_2 = nn.MaxPool1d(5)\n",
    "\n",
    "        # Final fully connected linear layer from the 50 output channels to the\n",
    "        # 6 sentiment categories \n",
    "        self.linear_layer = nn.Linear(50, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Defining the forward pass of an input batch x.\n",
    "\n",
    "        Args:\n",
    "            x, tensor: The input is a batch of tweets from the data.\n",
    "        Returns:\n",
    "            y, float: The output are the logits from the final layer.\n",
    "        \"\"\"\n",
    "        # x will correspond here to a batch; therefore, the input dimensions of \n",
    "        # the embedding will be by PyTorch convention as follows:\n",
    "        # [batch_size, seq_len, emb_dim]\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Unfortunately the embedding tensor does not correspond to the shape \n",
    "        # that is needed for nn.Conv1d(); for this reason, we must switch its \n",
    "        # order to [batch_size, emb_dim, seq_len] for PyTorch\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # We can wrap the ReLu activation function around our convolution layer\n",
    "        # The output tensor will have the following shape: \n",
    "        # [batch_size, 100, seq_len]\n",
    "        x = nn.functional.relu(self.conv_1(x))\n",
    "\n",
    "        # Applying max pooling of size 3 means that the output length of the \n",
    "        # sequence is shrunk to seq_len//3\n",
    "        x = self.pool_1(x)\n",
    "\n",
    "        # Output of the following layer: [batch_size, 50, seq_len//3]\n",
    "        x = nn.functional.relu(self.conv_2(x))\n",
    "\n",
    "        # Shrinking the sequence length by 5\n",
    "        x = self.pool_2(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # At this point we have a tensor with 3 dimensions; however, the final layer \n",
    "        # requires an input of size [batch_size x 50]. To get this value we can \n",
    "        # aggregate the values and continue only with their mean\n",
    "        x = x.mean(dim=-1)\n",
    "\n",
    "        # In this fasion, the linear layer can be used to make predictions\n",
    "        y = self.linear_layer(x)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def fit(self, train_instances, val_instances, epochs, batch_size):\n",
    "        \"\"\" \n",
    "        Gradient based fitting method with Adam optimization and automatic \n",
    "        evaluation (F1 score) for each epoch.\n",
    "\n",
    "        Args:\n",
    "            train_instances, list: List of instance tuples.\n",
    "            val_instances, list: List of instance tuples.\n",
    "            epochs, int: Number of training epochs.\n",
    "            batch_size, int: Number of batch size.\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_batches = batching(\n",
    "                train_instances,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True)\n",
    "            \n",
    "            for inputs, labels in tqdm(train_batches):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = nn.functional.cross_entropy(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            train_f1 = self.evaluate(train_instances, batch_size=batch_size)\n",
    "            val_f1 = self.evaluate(val_instances, batch_size=batch_size)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1} train F1 score: {train_f1}, validation F1 score: {val_f1}\")\n",
    "\n",
    "    def predict(self, input):\n",
    "        \"\"\" \n",
    "        To make inferences from the model.\n",
    "\n",
    "        Args:\n",
    "            input, tensor: Single instance.\n",
    "        Returns:\n",
    "            int: Integer for most probable class.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        outputs = self(input)\n",
    "\n",
    "        return torch.argmax(outputs, dim=-1)\n",
    "\n",
    "    def evaluate(self, instances, batch_size):\n",
    "        \"\"\" \n",
    "        To make evaluations against the gold standard (true labels) from the \n",
    "        data.\n",
    "\n",
    "        Args:\n",
    "            instances, list: List of instance tuples.\n",
    "            batch_size, int: Batch size.\n",
    "        Returns:\n",
    "            float: Macro F1 score for given instances.\n",
    "        \"\"\"\n",
    "        batches = batching(instances, batch_size=batch_size, shuffle=False)\n",
    "        true = []\n",
    "        pred = []\n",
    "\n",
    "        for inputs, labels in batches:\n",
    "            true.extend(labels)\n",
    "            pred.extend(self.predict(inputs))\n",
    "\n",
    "        return f1_score(true, pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf02f634-df1b-498f-a9e2-b46b89ba98a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:03<00:00, 290.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train F1 score: 0.6376884918322829, validation F1 score: 0.5924978680935198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:03<00:00, 302.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train F1 score: 0.9129537969487321, validation F1 score: 0.8439630888619769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:03<00:00, 307.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train F1 score: 0.9628621961987808, validation F1 score: 0.8648452562113952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:03<00:00, 303.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train F1 score: 0.9778829605265137, validation F1 score: 0.8677752142252126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:03<00:00, 301.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train F1 score: 0.9768250196328975, validation F1 score: 0.8757831856530034\n"
     ]
    }
   ],
   "source": [
    "classifier = CNN_Classifier()\n",
    "classifier.fit(train_instances, val_instances, epochs=5, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3b6ec90-a70f-4fb7-b02c-1261f9d563c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for test set: 0.8561850096890805\n"
     ]
    }
   ],
   "source": [
    "f1_test = classifier.evaluate(test_instances, batch_size=16)\n",
    "print(f\"F1 score for test set: {f1_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec18f2b-5ba1-466b-9442-fd2f2cdc8be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
