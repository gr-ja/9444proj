{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d018716b-0cb0-43f3-abfd-ace6f613534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from tokenizers import Tokenizer, models, trainers\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e5d4d27-6fcc-4e38-ad99-5aedad8842fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = load_dataset(\"dair-ai/emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0315fef-7e42-4278-8dd5-6078ec8b754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c66816cb-e29d-4940-95bc-b785747447e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = emotions[\"train\"]\n",
    "validation_data = emotions[\"validation\"]\n",
    "test_data = emotions[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b2a4d5-ea1c-4196-844c-fddd2a6f2259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "vocab_n = 5000\n",
    "sequence_len = 64\n",
    "\n",
    "# Initialize a tokenizer using BPE (Byte Pair Encoding)\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.enable_padding(length=sequence_len)\n",
    "tokenizer.enable_truncation(max_length=sequence_len)\n",
    "tokenizer_trainer = trainers.BpeTrainer(vocab_size=vocab_n)\n",
    "tokenizer.train_from_iterator(train_data[\"text\"], trainer=tokenizer_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8b3d861-7e4f-48d3-b643-06e9b9a88405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, tokenizer: Tokenizer):\n",
    "    \"\"\" \n",
    "    Helper function to tokenize text and return corresponding token IDs as tensors.\n",
    "\n",
    "    Args:\n",
    "        text, str: Text instance from training data.\n",
    "        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.\n",
    "    Returns:\n",
    "        Tensor: One-dimensional PyTorch tensor with token IDs.\n",
    "    \"\"\"\n",
    "    return torch.tensor(tokenizer.encode(text).ids)\n",
    "\n",
    "\n",
    "def preprocess_label(label: int):\n",
    "    \"\"\" \n",
    "    Helper function to return label as tensor.\n",
    "\n",
    "    Args:\n",
    "        label, int: Label from instance.\n",
    "    Returns:\n",
    "        Tensor: One-dimensional PyTorch tensor containing the label index.\n",
    "    \"\"\"\n",
    "    return torch.tensor(label)\n",
    "\n",
    "\n",
    "def preprocess(data: dict, tokenizer: Tokenizer):\n",
    "    \"\"\" \n",
    "    Transforms input dataset to tokenized vector representations.\n",
    "\n",
    "    Args:\n",
    "        data, dict: Dictionary with text instances and labels.\n",
    "        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.\n",
    "    Returns:\n",
    "        list: List with tensors for the input texts and labels.\n",
    "    \"\"\"\n",
    "    instances = []\n",
    "\n",
    "    for text, label in zip(data[\"text\"], data[\"label\"]):\n",
    "        input = preprocess_text(text, tokenizer)\n",
    "        label = preprocess_label(label)\n",
    "        \n",
    "        instances.append((input, label))\n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85779c8a-cb93-472c-8e61-8e7aadf46fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances = preprocess(train_data, tokenizer)\n",
    "val_instances = preprocess(validation_data, tokenizer)\n",
    "test_instances = preprocess(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba01debf-56ad-4302-aa85-8b506e686a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching\n",
    "\n",
    "def batching(instances: list, batch_size: int, shuffle: bool):\n",
    "    \"\"\" \n",
    "    Batches input instances along the given size and returns list of batches.\n",
    "\n",
    "    Args:\n",
    "        instances, list: List of instances, containing a tuple of two tensors \n",
    "            for each text as well as corresponding label.\n",
    "        batch_size, int: Size for batches.\n",
    "        shuffle, bool: If true, the instances will be shuffled before batching.\n",
    "    Returns:\n",
    "        list: List containing tuples that correspond to single batches.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        random.shuffle(instances)\n",
    "\n",
    "    batches = []\n",
    "\n",
    "    # We iterate through the instances with batch_size steps\n",
    "    for i in range(0, len(instances), batch_size):\n",
    "\n",
    "        # Stacking the instances with dim=0 (default value)\n",
    "        batch_texts = torch.stack(\n",
    "            [instance[0] for instance in instances[i : i + batch_size]]\n",
    "        )\n",
    "        batch_labels = torch.stack(\n",
    "            [instance[1] for instance in instances[i : i + batch_size]]\n",
    "        )\n",
    "\n",
    "        batches.append((batch_texts, batch_labels))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5aa7e1e-9735-4c8d-8761-68d251b2036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Network\n",
    "\n",
    "class CNN_Classifier(nn.Module):\n",
    "    \"\"\" \n",
    "    CNN for sentiment classification with 6 classes, consisting of an embedding \n",
    "    layer, two convolutional layers with different filter sizes, different \n",
    "    pooling sizes, as well as one linear output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # We can implement embeddings as a simple lookup-table for given word \n",
    "        # indices\n",
    "        self.embedding = nn.Embedding(tokenizer.get_vocab_size(), 300)\n",
    "\n",
    "        # One-dimensional convolution-layer with 300 input channels, and 100  \n",
    "        # output channels as well as kernel size of 3; note that the\n",
    "        # one-dimensional convolutional layer has 3 dimensions\n",
    "        self.conv_1 = nn.Conv1d(300, 100, 3, padding=\"same\")\n",
    "\n",
    "        # Pooling with with a one-dimensional sliding window of length 3, \n",
    "        # reducing in this fashion the sequence length \n",
    "        self.pool_1 = nn.MaxPool1d(3)\n",
    "\n",
    "        # The input will be the reduced number of maximum picks from the\n",
    "        # previous operation; the dimension of those picks is the same as the\n",
    "        # output channel size from self.conv_1. We apply a different filter of \n",
    "        # size 5.\n",
    "        self.conv_2 = nn.Conv1d(100, 50, 5, padding=\"same\")\n",
    "\n",
    "        # Pooling with window size of 5\n",
    "        self.pool_2 = nn.MaxPool1d(5)\n",
    "\n",
    "        # Final fully connected linear layer from the 50 output channels to the\n",
    "        # 6 sentiment categories \n",
    "        self.linear_layer = nn.Linear(50, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Defining the forward pass of an input batch x.\n",
    "\n",
    "        Args:\n",
    "            x, tensor: The input is a batch of tweets from the data.\n",
    "        Returns:\n",
    "            y, float: The output are the logits from the final layer.\n",
    "        \"\"\"\n",
    "        # x will correspond here to a batch; therefore, the input dimensions of \n",
    "        # the embedding will be by PyTorch convention as follows:\n",
    "        # [batch_size, seq_len, emb_dim]\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Unfortunately the embedding tensor does not correspond to the shape \n",
    "        # that is needed for nn.Conv1d(); for this reason, we must switch its \n",
    "        # order to [batch_size, emb_dim, seq_len] for PyTorch\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # We can wrap the ReLu activation function around our convolution layer\n",
    "        # The output tensor will have the following shape: \n",
    "        # [batch_size, 100, seq_len]\n",
    "        x = nn.functional.relu(self.conv_1(x))\n",
    "\n",
    "        # Applying max pooling of size 3 means that the output length of the \n",
    "        # sequence is shrunk to seq_len//3\n",
    "        x = self.pool_1(x)\n",
    "\n",
    "        # Output of the following layer: [batch_size, 50, seq_len//3]\n",
    "        x = nn.functional.relu(self.conv_2(x))\n",
    "\n",
    "        # Shrinking the sequence length by 5\n",
    "        x = self.pool_2(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # At this point we have a tensor with 3 dimensions; however, the final layer \n",
    "        # requires an input of size [batch_size x 50]. To get this value we can \n",
    "        # aggregate the values and continue only with their mean\n",
    "        x = x.mean(dim=-1)\n",
    "\n",
    "        # In this fasion, the linear layer can be used to make predictions\n",
    "        y = self.linear_layer(x)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def fit(self, train_instances, val_instances, epochs, batch_size):\n",
    "        \"\"\" \n",
    "        Gradient based fitting method with Adam optimization and automatic \n",
    "        evaluation (F1 score) for each epoch.\n",
    "\n",
    "        Args:\n",
    "            train_instances, list: List of instance tuples.\n",
    "            val_instances, list: List of instance tuples.\n",
    "            epochs, int: Number of training epochs.\n",
    "            batch_size, int: Number of batch size.\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_batches = batching(\n",
    "                train_instances,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True)\n",
    "            \n",
    "            for inputs, labels in tqdm(train_batches):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = nn.functional.cross_entropy(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            train_f1 = self.evaluate(train_instances, batch_size=batch_size)\n",
    "            val_f1 = self.evaluate(val_instances, batch_size=batch_size)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1} train F1 score: {train_f1}, validation F1 score: {val_f1}\")\n",
    "\n",
    "    def predict(self, input):\n",
    "        \"\"\" \n",
    "        To make inferences from the model.\n",
    "\n",
    "        Args:\n",
    "            input, tensor: Single instance.\n",
    "        Returns:\n",
    "            int: Integer for most probable class.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        outputs = self(input)\n",
    "\n",
    "        return torch.argmax(outputs, dim=-1)\n",
    "\n",
    "    def evaluate(self, instances, batch_size):\n",
    "        \"\"\" \n",
    "        To evaluate model's performance by various processes/standard.\n",
    "\n",
    "        Args:\n",
    "            instances, list: List of instance tuples.\n",
    "            batch_size, int: Batch size.\n",
    "        Returns:\n",
    "            float: Macro F1 score for given instances.\n",
    "        \"\"\"\n",
    "        batches = batching(instances, batch_size=batch_size, shuffle=False)\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "\n",
    "        for inputs, labels in batches:\n",
    "            y_test.extend(labels)\n",
    "            y_pred.extend(self.predict(inputs))\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "        recall = recall_score(y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        print(\"CNN Classifier:\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1 Score: {f1}\")\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "        print(f\"Classification Report:\\n{report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf02f634-df1b-498f-a9e2-b46b89ba98a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 283.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Classifier:\n",
      "Accuracy: 0.8441875\n",
      "Precision: 0.7841844527850768\n",
      "Recall: 0.6722731568191591\n",
      "F1 Score: 0.6703932542614649\n",
      "Confusion Matrix:\n",
      "[[4379   86   28  126   47    0]\n",
      " [  60 5139   93   32   38    0]\n",
      " [  31  493  603  161   16    0]\n",
      " [ 137  126  125 1691   78    2]\n",
      " [  47  116   18   71 1682    3]\n",
      " [  14   72   82  129  262   13]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94      4666\n",
      "           1       0.85      0.96      0.90      5362\n",
      "           2       0.64      0.46      0.54      1304\n",
      "           3       0.77      0.78      0.77      2159\n",
      "           4       0.79      0.87      0.83      1937\n",
      "           5       0.72      0.02      0.04       572\n",
      "\n",
      "    accuracy                           0.84     16000\n",
      "   macro avg       0.78      0.67      0.67     16000\n",
      "weighted avg       0.84      0.84      0.83     16000\n",
      "\n",
      "CNN Classifier:\n",
      "Accuracy: 0.7945\n",
      "Precision: 0.6992531012715045\n",
      "Recall: 0.6217595877320382\n",
      "F1 Score: 0.6178005487994881\n",
      "Confusion Matrix:\n",
      "[[492  22   3  21  12   0]\n",
      " [  7 665  20   7   5   0]\n",
      " [  5  73  69  26   5   0]\n",
      " [ 25  17  19 200  13   1]\n",
      " [ 18  18   1  13 162   0]\n",
      " [  2  19   8  20  31   1]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90       550\n",
      "           1       0.82      0.94      0.88       704\n",
      "           2       0.57      0.39      0.46       178\n",
      "           3       0.70      0.73      0.71       275\n",
      "           4       0.71      0.76      0.74       212\n",
      "           5       0.50      0.01      0.02        81\n",
      "\n",
      "    accuracy                           0.79      2000\n",
      "   macro avg       0.70      0.62      0.62      2000\n",
      "weighted avg       0.78      0.79      0.77      2000\n",
      "\n",
      "Epoch 1 train F1 score: None, validation F1 score: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 289.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Classifier:\n",
      "Accuracy: 0.9520625\n",
      "Precision: 0.9378671105240146\n",
      "Recall: 0.9148365697550123\n",
      "F1 Score: 0.9256561471110906\n",
      "Confusion Matrix:\n",
      "[[4633    7    2   11   13    0]\n",
      " [  52 5201   79   11   12    7]\n",
      " [  13  120 1142   27    1    1]\n",
      " [ 120   14    4 2010   11    0]\n",
      " [  44   11    1   41 1793   47]\n",
      " [   5    6    2   19   86  454]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      4666\n",
      "           1       0.97      0.97      0.97      5362\n",
      "           2       0.93      0.88      0.90      1304\n",
      "           3       0.95      0.93      0.94      2159\n",
      "           4       0.94      0.93      0.93      1937\n",
      "           5       0.89      0.79      0.84       572\n",
      "\n",
      "    accuracy                           0.95     16000\n",
      "   macro avg       0.94      0.91      0.93     16000\n",
      "weighted avg       0.95      0.95      0.95     16000\n",
      "\n",
      "CNN Classifier:\n",
      "Accuracy: 0.8825\n",
      "Precision: 0.865280914344853\n",
      "Recall: 0.8256063274390747\n",
      "F1 Score: 0.8428768624783659\n",
      "Confusion Matrix:\n",
      "[[532   8   1   2   6   1]\n",
      " [ 22 646  18   8   8   2]\n",
      " [ 10  29 126   9   3   1]\n",
      " [ 30   3   3 237   2   0]\n",
      " [ 19   5   0  15 166   7]\n",
      " [  4   5   1   2  11  58]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91       550\n",
      "           1       0.93      0.92      0.92       704\n",
      "           2       0.85      0.71      0.77       178\n",
      "           3       0.87      0.86      0.86       275\n",
      "           4       0.85      0.78      0.81       212\n",
      "           5       0.84      0.72      0.77        81\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.87      0.83      0.84      2000\n",
      "weighted avg       0.88      0.88      0.88      2000\n",
      "\n",
      "Epoch 2 train F1 score: None, validation F1 score: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 281.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Classifier:\n",
      "Accuracy: 0.9784375\n",
      "Precision: 0.9606572599694783\n",
      "Recall: 0.9752671190911865\n",
      "F1 Score: 0.967426288564884\n",
      "Confusion Matrix:\n",
      "[[4575   27    1   42   20    1]\n",
      " [   2 5296   52    1    1   10]\n",
      " [   0   37 1261    5    0    1]\n",
      " [  15    8    0 2115   21    0]\n",
      " [   8    7    0   11 1845   66]\n",
      " [   0    0    0    2    7  563]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      4666\n",
      "           1       0.99      0.99      0.99      5362\n",
      "           2       0.96      0.97      0.96      1304\n",
      "           3       0.97      0.98      0.98      2159\n",
      "           4       0.97      0.95      0.96      1937\n",
      "           5       0.88      0.98      0.93       572\n",
      "\n",
      "    accuracy                           0.98     16000\n",
      "   macro avg       0.96      0.98      0.97     16000\n",
      "weighted avg       0.98      0.98      0.98     16000\n",
      "\n",
      "CNN Classifier:\n",
      "Accuracy: 0.895\n",
      "Precision: 0.8652014880696334\n",
      "Recall: 0.8735947913947775\n",
      "F1 Score: 0.8678273162062916\n",
      "Confusion Matrix:\n",
      "[[499  22   2  15  11   1]\n",
      " [  1 659  26   7   4   7]\n",
      " [  3  33 135   4   1   2]\n",
      " [ 14   6   3 249   3   0]\n",
      " [  9   5   0  12 174  12]\n",
      " [  1   3   1   0   2  74]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93       550\n",
      "           1       0.91      0.94      0.92       704\n",
      "           2       0.81      0.76      0.78       178\n",
      "           3       0.87      0.91      0.89       275\n",
      "           4       0.89      0.82      0.86       212\n",
      "           5       0.77      0.91      0.84        81\n",
      "\n",
      "    accuracy                           0.90      2000\n",
      "   macro avg       0.87      0.87      0.87      2000\n",
      "weighted avg       0.90      0.90      0.89      2000\n",
      "\n",
      "Epoch 3 train F1 score: None, validation F1 score: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 273.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Classifier:\n",
      "Accuracy: 0.983875\n",
      "Precision: 0.9794597475800645\n",
      "Recall: 0.9782706716313972\n",
      "F1 Score: 0.9786800064823923\n",
      "Confusion Matrix:\n",
      "[[4652    2    1    3    8    0]\n",
      " [  10 5324   21    0    4    3]\n",
      " [   3   46 1255    0    0    0]\n",
      " [  67    3    1 2051   37    0]\n",
      " [  19    0    0    0 1894   24]\n",
      " [   1    0    0    0    5  566]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      4666\n",
      "           1       0.99      0.99      0.99      5362\n",
      "           2       0.98      0.96      0.97      1304\n",
      "           3       1.00      0.95      0.97      2159\n",
      "           4       0.97      0.98      0.98      1937\n",
      "           5       0.95      0.99      0.97       572\n",
      "\n",
      "    accuracy                           0.98     16000\n",
      "   macro avg       0.98      0.98      0.98     16000\n",
      "weighted avg       0.98      0.98      0.98     16000\n",
      "\n",
      "CNN Classifier:\n",
      "Accuracy: 0.894\n",
      "Precision: 0.8754740048891875\n",
      "Recall: 0.8584905183765908\n",
      "F1 Score: 0.8654762941888398\n",
      "Confusion Matrix:\n",
      "[[531   6   3   2   7   1]\n",
      " [ 15 649  23   2  11   4]\n",
      " [  3  39 132   1   2   1]\n",
      " [ 28   7   2 231   7   0]\n",
      " [ 21   3   0   2 176  10]\n",
      " [  2   5   1   0   4  69]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.92       550\n",
      "           1       0.92      0.92      0.92       704\n",
      "           2       0.82      0.74      0.78       178\n",
      "           3       0.97      0.84      0.90       275\n",
      "           4       0.85      0.83      0.84       212\n",
      "           5       0.81      0.85      0.83        81\n",
      "\n",
      "    accuracy                           0.89      2000\n",
      "   macro avg       0.88      0.86      0.87      2000\n",
      "weighted avg       0.90      0.89      0.89      2000\n",
      "\n",
      "Epoch 4 train F1 score: None, validation F1 score: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 273.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Classifier:\n",
      "Accuracy: 0.9879375\n",
      "Precision: 0.9800485966066724\n",
      "Recall: 0.9850867882605495\n",
      "F1 Score: 0.9824329897222192\n",
      "Confusion Matrix:\n",
      "[[4658    0    1    6    0    1]\n",
      " [   9 5300   28   13    4    8]\n",
      " [   0   18 1279    6    0    1]\n",
      " [  13    1    0 2141    4    0]\n",
      " [  28    0    0   24 1863   22]\n",
      " [   0    0    0    0    6  566]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      4666\n",
      "           1       1.00      0.99      0.99      5362\n",
      "           2       0.98      0.98      0.98      1304\n",
      "           3       0.98      0.99      0.98      2159\n",
      "           4       0.99      0.96      0.98      1937\n",
      "           5       0.95      0.99      0.97       572\n",
      "\n",
      "    accuracy                           0.99     16000\n",
      "   macro avg       0.98      0.99      0.98     16000\n",
      "weighted avg       0.99      0.99      0.99     16000\n",
      "\n",
      "CNN Classifier:\n",
      "Accuracy: 0.895\n",
      "Precision: 0.8618082884772917\n",
      "Recall: 0.8689309923675944\n",
      "F1 Score: 0.863326887868015\n",
      "Confusion Matrix:\n",
      "[[524   5   4  15   1   1]\n",
      " [ 12 639  29  12   6   6]\n",
      " [  2  26 139  10   0   1]\n",
      " [ 15   3   2 251   4   0]\n",
      " [ 13   3   0  17 166  13]\n",
      " [  2   2   1   2   3  71]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94       550\n",
      "           1       0.94      0.91      0.92       704\n",
      "           2       0.79      0.78      0.79       178\n",
      "           3       0.82      0.91      0.86       275\n",
      "           4       0.92      0.78      0.85       212\n",
      "           5       0.77      0.88      0.82        81\n",
      "\n",
      "    accuracy                           0.90      2000\n",
      "   macro avg       0.86      0.87      0.86      2000\n",
      "weighted avg       0.90      0.90      0.90      2000\n",
      "\n",
      "Epoch 5 train F1 score: None, validation F1 score: None\n"
     ]
    }
   ],
   "source": [
    "classifier = CNN_Classifier()\n",
    "classifier.fit(train_instances, val_instances, epochs=5, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3b6ec90-a70f-4fb7-b02c-1261f9d563c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Classifier:\n",
      "Accuracy: 0.8965\n",
      "Precision: 0.845107185966377\n",
      "Recall: 0.8559161416862501\n",
      "F1 Score: 0.8490755186618048\n",
      "Confusion Matrix:\n",
      "[[553  10   0  15   2   1]\n",
      " [  4 634  40   6   5   6]\n",
      " [  5  25 120   8   0   1]\n",
      " [ 13   6   0 252   1   3]\n",
      " [ 11   1   0  17 182  13]\n",
      " [  1   4   0   1   8  52]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95       581\n",
      "           1       0.93      0.91      0.92       695\n",
      "           2       0.75      0.75      0.75       159\n",
      "           3       0.84      0.92      0.88       275\n",
      "           4       0.92      0.81      0.86       224\n",
      "           5       0.68      0.79      0.73        66\n",
      "\n",
      "    accuracy                           0.90      2000\n",
      "   macro avg       0.85      0.86      0.85      2000\n",
      "weighted avg       0.90      0.90      0.90      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1_test = classifier.evaluate(test_instances, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec18f2b-5ba1-466b-9442-fd2f2cdc8be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
